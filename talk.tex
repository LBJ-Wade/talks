\documentclass[]{beamer}
\usepackage{graphicx} % For including single page pdfs
\usepackage{bm}       % bold math
\usepackage{pgffor}   % for loop
\usepackage{tikz}
\usepackage{multimedia}
\usepackage{layouts}
\usepackage{hyperref}



\newcommand{\lik}{\mathcal{L}}
\newcommand{\posterior}{\mathcal{P}}
\newcommand{\prior}{\pi}
\newcommand{\ev}{\mathcal{Z}}

\newcommand{\prob}{\mathrm{P}}

\newcommand{\PR}{\mathcal{P}_\mathcal{R}}
\newcommand{\Pknotj}[1]{\mathcal{P}_{#1}}
\newcommand{\Nknots}{N_\mathrm{knots}}

\newcommand{\movablecross}[1]{%
  \draw[->](#1) -- ++(0:\croslen);
  \draw[->](#1) -- ++(90:\croslen);
  \draw[->](#1) -- ++(180:\croslen);
  \draw[->](#1) -- ++(270:\croslen);
  \fill[red!70!black] (#1) circle (2pt);
}

\newcommand{\movablevert}[1]{%
  \draw[->](#1) -- ++(90:\croslen);
  \draw[->](#1) -- ++(270:\croslen);
  \fill[red!70!black] (#1) circle (2pt);
}





\setbeamertemplate{navigation symbols}{} % Turn off that bottom bar


\title{PolyChord \& the future of nested sampling}
\subtitle{Sampling, Parameter Estimation and Bayesian Model Comparison}
\author[Handley] % (optional, for multiple authors)
{Will Handley\\ \small{wh260@cam.ac.uk}}
\institute[University of Cambridge] % (optional)
{%
Supervisors: Anthony Lasenby \& Mike Hobson \\
Astrophysics Group \\
Cavendish Laboratory \\
University of Cambridge
}
\date{March 8, 2016}



\begin{document}

\begin{frame}
  \titlepage
\end{frame}

\begin{frame}
  \tableofcontents
\end{frame}


\section{Parameter estimation \& model comparison}
%\begin{frame}
%  \frametitle{Notation} 
%
%  \begin{itemize}
%      \pause
%    \item Data: $D$
%      \pause
%    \item Model: $M$
%      \pause
%    \item Parameters: $\Theta$
%      \pause
%    \item Likelihood: $\prob(D|\Theta,M)=\lik(\Theta)$
%      \pause
%    \item Posterior: $\prob(\Theta|D,M)=\posterior(\Theta)$
%      \pause
%    \item Prior: $\prob(\Theta|M)=\prior(\Theta)$
%      \pause
%    \item Evidence: $\prob(D|M)=\ev$
%  \end{itemize}
%
%
%\end{frame}

\begin{frame}
  \frametitle{What is nested sampling?}
  \begin{itemize}
    \item<2-> Nested sampling is an alternative sampling approach. 
    \item<3-> Unlike traditional methods, it does not sample from the posterior directly.
    \item<4-> Instead it gradually compresses the prior onto the posterior.
    \item<5-> In doing so, it circumvents many issues (dimensionality, topology, geometry) that beset normal approaches.
    \item<5-> Similar to simulated annealing, but automatically picks the ``correct'' annealing schedule.
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Bayes' theorem}
  \framesubtitle{Parameter estimation}

  \pause
  What does data tell us about the params $\Theta$ of our model $M$?
    \[\prob(\Theta|D,M) = \frac{\prob(D|\Theta,M) \prob(\Theta|M) }{ \prob(D|M) }\] 
    \pause
    \[\text{Posterior } = \frac{\text{Likelihood} \times \text{Prior} } {\text{Evidence} }\] 
    \pause
    \[ \posterior(\Theta)= \frac{\lik(\Theta) \prior(\Theta)} {\ev}\] 

\end{frame}

\begin{frame}
  \frametitle{Bayes' theorem}
  \framesubtitle{Model comparison}
  \pause
  What does data tell us about our model $M_i$ in relation to other models $\{M_1,M_2,\cdots\}$?
  \pause
    \[\prob(M_i|D) = \frac{\prob(D|M_i) \prob(M_i) }{ \prob(D) }\] 

    \pause
    \[\prob(M_i|D) = \frac{\ev_i \: \mu_i}{\sum_k \ev_k \: \mu_k}\] 
\end{frame}

\begin{frame}
  \frametitle{Parameter estimation \& model comparison} 
  \framesubtitle{The challenge}

  \pause
  Both of these are challenging things to compute.
  \begin{itemize}
      \pause
    \item Markov-Chain Monte-Carlo (MCMC) can solve the first of these (kind of)
      \pause
    \item Nested sampling (NS) promises to solve both simultaneously.
  \end{itemize}
 
\end{frame}

\begin{frame}
  \frametitle{Parameter estimation \& model comparison} 
  \framesubtitle{Why is it difficult?} 

	\begin{columns}
	\begin{column}{0.5\textwidth}
		\begin{enumerate}
          \item<2-> In high dimensions, posterior $\posterior$ occupies a vanishingly small region of the prior $\prior$.
          \item<3-> Worse, you don't know where this region is.
		\end{enumerate}
	\end{column}
	\begin{column}{0.5\textwidth}
		\includegraphics<1->[width=\textwidth]{figures/posteriors}	
	\end{column}
	\end{columns}
    \begin{itemize}
      \item<4-> Describing an $N$-dimensional posterior fully is impossible.
      \item<5-> {\em Sampling} the posterior is an excellent compression scheme.
    \end{itemize}
 
\end{frame}


\section{Metropolis Hastings}

\begin{frame}
  \frametitle{Markov-Chain Monte-Carlo (MCMC)} 
  \framesubtitle{Metropolis-Hastings, Gibbs, Hamiltonian\ldots} 
  \begin{itemize}
      \pause
    \item Turn the $N$-dimensional problem into a one-dimensional one.
      \pause
    \item Explore the space via a biased random walk.
      \begin{enumerate}
          \pause
        \item Pick random direction
          \pause
        \item Choose step length
          \pause
        \item If uphill, make step\ldots
          \pause
        \item \ldots otherwise sometimes make step. 
      \end{enumerate}
  \end{itemize}
 
\end{frame}

\begin{frame}
  \frametitle{MCMC in action} 
  \movie[width=\textwidth,height=0.52\textwidth]{%
  \includegraphics[width=\textwidth]{movies/MCMC_0.pdf}}{movies/MCMC.mp4}
\end{frame}
\begin{frame}
  \frametitle{MCMC in action} 
  \includegraphics[width=\textwidth]{movies/MCMC_1.pdf}
\end{frame}

\begin{frame}
  \frametitle{When MCMC fails} 
  \framesubtitle{Burn in} 
  \movie[width=\textwidth,height=0.52\textwidth]{%
  \includegraphics[width=\textwidth]{movies/MCMC_burn_in_0.pdf}}{movies/MCMC_burn_in.mp4}
\end{frame}
\begin{frame}
  \frametitle{When MCMC fails} 
  \framesubtitle{Burn in} 
  \includegraphics[width=\textwidth]{movies/MCMC_burn_in_1.pdf}
\end{frame}

\begin{frame}
  \frametitle{When MCMC fails} 
  \framesubtitle{Tuning the proposal distribution} 
  \movie[width=\textwidth,height=0.52\textwidth]{%
  \includegraphics[width=\textwidth]{movies/MCMC_proposal_0.pdf}}{movies/MCMC_proposal.mp4}
\end{frame}
\begin{frame}
  \frametitle{When MCMC fails} 
  \framesubtitle{Tuning the proposal distribution} 
  \includegraphics[width=\textwidth]{movies/MCMC_proposal_1.pdf}
\end{frame}

\begin{frame}
  \frametitle{When MCMC fails} 
  \framesubtitle{Multimodality} 
  \movie[width=\textwidth,height=0.52\textwidth]{%
  \includegraphics[width=\textwidth]{movies/MCMC_multimodal_0.pdf}}{movies/MCMC_multimodal.mp4}
\end{frame}
\begin{frame}
  \frametitle{When MCMC fails} 
  \framesubtitle{Multimodality} 
  \includegraphics[width=\textwidth]{movies/MCMC_multimodal_1.pdf}
\end{frame}

\begin{frame}
  \frametitle{When MCMC fails} 
  \framesubtitle{Phase transitions} 

  \includegraphics[width=\textwidth]{figures/phase_transition}
 
\end{frame}

\begin{frame}
  \frametitle{When MCMC fails} 
  \framesubtitle{The real reason\ldots} 

  \begin{itemize}
    \item<2-> MCMC does not give you evidences!
  \end{itemize}

  \begin{align}
    \onslide<3->{%
    \ev 
    &= \prob(D|M) 
    \nonumber\\
  }
    \onslide<4->{%
    &= \int\prob(D|\Theta,M)\prob(\Theta|M) d\Theta 
    \nonumber\\
  }
    \onslide<5->{%
    &= \int\lik(\Theta)\prior(\Theta) d\Theta 
    \nonumber\\
  }
    \onslide<6->{%
    &= \left\langle \lik \right\rangle_\prior
    \nonumber
  }
  \end{align}
  
  \begin{itemize}
    \item<7-> MCMC fundamentally explores the posterior, and cannot average over the prior.
    \item<8-> Simulated annealing gives one possibility for computing evidences.
      \begin{itemize}
        \item<9-> Suffers from similar issues to MCMC, especially phase transitions 
      \end{itemize}
  \end{itemize}
 
\end{frame}

\section{Nested Sampling}
\begin{frame}
  \frametitle{Nested Sampling} 
  \framesubtitle{John Skilling's alternative to MCMC!} 

  \pause
  New procedure: 

  \pause
  Maintain a set $S$ of $n$ samples, which are sequentially updated:

  \begin{description}
      \pause
    \item[$S_0$:] Generate $n$ samples from the prior $\prior$. 
      \pause
    \item[$S_{n+1}$:] Delete the lowest likelihood sample in $S_{n}$, and replace it with a new sample with higher likelihood
  \end{description}

  \pause
  Requires one to be able to sample from the prior, subject to a {\em hard likelihood constraint}.

\end{frame}




\begin{frame}
  \frametitle{Nested Sampling}
  \framesubtitle{Graphical aid}
\foreach \pagenum in {1,...,38} {%
  \includegraphics<\pagenum>[width=\textwidth,page=\pagenum]{figures/nested_sampling}
}
\end{frame}



\begin{frame}
  \frametitle{Calculating evidences}

  \begin{itemize}
    \item<2-> \onslide<3->{Transform to 1 dimensional integral} \onslide<4->{$\pi(\theta)d\theta = dX$}
      \onslide<2->{\[ 
        \ev = \int \lik(\theta)\pi(\theta)d\theta \onslide<5->{= \int \lik(X)dX}
      \] }
    \item<6-> $X$ is the {\em prior volume}
      \onslide<7->{%
        \[ X(\lik) = \int_{\lik(\theta)>\lik} \pi(\theta) d\theta \]
      }
    \item<8-> i.e.\ the fraction of the prior which the iso-likelihood contour $\lik$ encloses.
  \end{itemize}

\end{frame}

\begin{frame}
  \frametitle{Nested Sampling}
  \framesubtitle{Calculating evidences}
  \includegraphics[width=\textwidth,page=38]{figures/nested_sampling}
\end{frame}

\begin{frame}
  \frametitle{Nested Sampling}
  \framesubtitle{Calculating evidences}

\foreach \pagenum in {1,...,5} {%
  \includegraphics<\pagenum>[width=\textwidth,page=\pagenum]{figures/nested_sampling_sum}
}
\end{frame}


\begin{frame}
  \frametitle{Nested Sampling} 
  \framesubtitle{Exponential volume contraction} 
  
  \begin{itemize}
      \pause
    \item At each iteration, the likelihood contour will shrink in volume by a factor of $\approx 1/n$.
      \pause
    \item Nested sampling zooms in to the peak of the posterior {\em exponentially}.
      \pause
    \item 
      \begin{equation}
        \ev \approx \sum_i \lik_i (X_{i-1} - X_{i})
      \end{equation}
      \pause
    \item 
      \begin{equation}
        X_{i+1} \approx \frac{n}{n+1}X_i, \qquad X_{0} = 1
      \end{equation}
  \end{itemize}
  
\end{frame}

%\begin{frame}
%  \frametitle{Estimating evidences} 
%  \framesubtitle{Evidence error} 
%
%\foreach \pagenum in {1,...,9} {%
%  \includegraphics<\pagenum>[width=\textwidth,page=\pagenum]{figures/areas}
%}
% 
%\end{frame}
%
%\begin{frame}
%  \frametitle{Estimating evidences} 
%  \framesubtitle{Evidence error} 
%
%
%  \begin{itemize}
%    \item<2-> approximate compression:
%  \end{itemize}
%  \[ 
%    \onslide<2->{\Delta \log X \sim -\frac{1}{n}} 
%    \onslide<3->{\pm \frac{1}{n}} 
%  \]
%  \[ 
%    \onslide<4->{\log X_i \sim -\frac{i}{n}}
%    \onslide<5->{\pm \frac{\sqrt{i}}{n}} 
%  \]
%  \onslide<6->{%
%  \begin{itemize}
%    \item<6-> \# of steps to get to $H$:
%  \end{itemize}
%  \[ i_H \sim n H \]
%  }
%  \onslide<7->{%
%  \begin{itemize}
%    \item estimate of volume at $H$:
%  \end{itemize}
%  \[ \log X_H \approx -H \pm \sqrt{\frac{H}{n}} \]
%}
%
%  \onslide<8->{%
%  \begin{itemize}
%    \item estimate of evidence error:
%  \end{itemize}
%  \[ \log\ev \approx \sum w_i \lik_i  \pm \sqrt{\frac{H}{n}} \]
%}
%
%\end{frame}


\begin{frame}
  \frametitle{Nested sampling} 
  \framesubtitle{Parameter estimation} 

  \begin{itemize}
      \pause
    \item NS can also be used to sample the posterior
      \pause
    \item The set of dead points are posterior samples with an appropriate weighting factor
  \end{itemize}
 
\end{frame}


\begin{frame}
  \frametitle{When NS succeeds} 
  \movie[width=\textwidth,height=0.52\textwidth]{%
  \includegraphics[width=\textwidth]{movies/NS_0.pdf}}{movies/NS.mp4}
\end{frame}
\begin{frame}
  \frametitle{When NS suceeds} 
  \includegraphics[width=\textwidth]{movies/NS_1.pdf}
\end{frame}

\begin{frame}
  \frametitle{When NS succeeds} 
  \movie[width=\textwidth,height=0.52\textwidth]{%
  \includegraphics[width=\textwidth]{movies/NS_multimodal_0.pdf}}{movies/NS_multimodal.mp4}
\end{frame}
\begin{frame}
  \frametitle{When NS suceeds} 
  \includegraphics[width=\textwidth]{movies/NS_multimodal_1.pdf}
\end{frame}


\begin{frame}
  \frametitle{Sampling from a hard likelihood constraint} 

  \pause
  \begin{quote}
    ``It is not the purpose of this introductory paper to develop the technology of navigation within such a volume. We merely note that exploring a hard-edged likelihood-constrained domain should prove to be neither more nor less demanding than exploring a likelihood-weighted space.''
    
   {\hfill --- John Skilling}
  \end{quote}

  \begin{itemize}
      \pause
    \item Most of the work in NS to date has been in attempting to implement a hard-edged sampler in the NS meta-algorithm.
  \end{itemize}
 
\end{frame}


\begin{frame}
  \frametitle{Sampling within an iso-likelihood contour}
  \framesubtitle{Previous attempts}


  \begin{description}
    \item<2->[Rejection Sampling] MultiNest; F.\ Feroz \& M.\ Hobson (2009).
      \begin{itemize}
        \item<3-> Suffers in high dimensions
      \end{itemize}
    \item<4->[Hamiltonian sampling] F.\ Feroz \& J.\ Skilling (2013).
      \begin{itemize}
        \item<5-> Requires gradients and tuning
      \end{itemize}
    \item<6->[Diffusion Nested Sampling] B.\ Brewer et al.\ (2009).
      \begin{itemize}
        \item<7-> Very promising
        \item<8-> Too many tuning parameters
      \end{itemize}
    %\item<8->[Slice Sampling] R.\ Neal (2000), Aitken \& Akman (2013).
  \end{description}

\end{frame}

\section{PolyChord}
\begin{frame}
  \frametitle{PolyChord}
  \framesubtitle{``Hit and run'' slice sampling}
\foreach \pagenum in {1,...,32} {%
  \includegraphics<\pagenum>[width=\textwidth,page=\pagenum]{figures/contour_step}
}
\end{frame}


\begin{frame}
  \frametitle{PolyChord}
  \framesubtitle{Key points}

  \begin{itemize}
    \item<2-> This procedure satisfies detailed balance.
      \begin{itemize}
        \item<3-> Works even if $\mathcal{L}_0$ contour is disjoint.
      \end{itemize}
    \item<4-> Need $N$ reasonably large $\sim\mathcal{O}(n_\mathrm{dims})$ so that $x_N$ is de-correlated from $x_1$.
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Issues with Slice Sampling}

  \begin{enumerate}
    \item<2-> Does not deal well with correlated distributions.
    \item<3-> Need to ``tune'' $w$ parameter.
  \end{enumerate}
\end{frame}

\begin{frame}
  \frametitle{PolyChord's solutions}
  \framesubtitle{Correlated distributions}

  \includegraphics[width=\textwidth]{figures/contour_transform}

\end{frame}


\begin{frame}
  \frametitle{PolyChord's solutions}
  \framesubtitle{Correlated distributions}

  \begin{itemize}
    \item<2-> We make an affine transformation to remove degeneracies, and ``whiten'' the space.
    \item<3-> Samples remain uniformly sampled
    \item<4-> We use the covariance matrix of the live points and all inter-chain points 
    \item<5-> Cholesky decomposition is the required skew transformation
    \item<6-> $w=1$ in this transformed space
  \end{itemize}
\end{frame}


\begin{frame}
  \frametitle{PolyChord's Additions}

  \begin{itemize}
    \item<2-> Parallelised up to number of live points with openMPI.
    \item<3-> Novel method for identifying and evolving modes separately.
    \item<4-> Implemented in CosmoMC, as ``CosmoChord'', with fast-slow parameters.
  \end{itemize}
\end{frame}






\section{Applications}

\begin{frame}
  \frametitle{PolyChord in action}
  \framesubtitle{Primordial power spectrum $\PR(k)$ reconstruction}


  \resizebox{\textwidth} {!} {%
    \begin{tikzpicture}
    % width of axes
      \def\xwidth{7}
      \def\ywidth{4}
    % min coordinate
      \def\xmn{0.5}
      \def\ymn{2}
    % start coordinate
      \def\xstart{2}
      \def\ystart{3}
    % middle coordinate
      \def\xmid{3}
      \def\ymid{1}
    % end coordinate
      \def\xend{5.5}
      \def\yend{3}
    % max coordinate
      \def\xmx{6.5}
      \def\ymx{1.5}

    % length of crosses
      \def\croslen{0.4}


    % Draw axes
      \draw [<->,thick] (0,\ywidth) node (yaxis) [above] {$\log\PR(k)$}
      |- (\xwidth,0) node (xaxis) [right] {$\log k$};
    % Draw limits
      %\draw [-,dashed] (\xmn,0) node[below] {$\log_{10}k_1$} -- (\xmn,\ywidth) ;
      %\draw [-,dashed] (\xmx,0) node[below] {$\log_{10}k_N$} -- (\xmx,\ywidth) ;

      \draw<1> (\xmn,\ymn) -- (\xmx,\ymx);
      \draw<1> (\xstart,\ystart) node[below right] {$A_s {\left(\frac{k}{k_*}\right)}^{n_s-1}$};

    % Draw the line joining start and end

      \coordinate (mn) at (\xmn,\ymn);
      \coordinate (start) at (\xstart,\ystart);
      \coordinate (mid) at (\xmid,\ymid);
      \coordinate (end) at (\xend,\yend);
      \coordinate (mx) at (\xmx,\ymx);
      \draw<2> (mn) -- (mx);
      \draw<2-> (mn) node[below right]    {$(k_1,\Pknotj{1})$};
      \draw<2> (mx) node[below left]     {$(k_{2},\Pknotj{{2}})$};
      \onslide<2->{\movablevert{mn}};
      \onslide<2->{\movablevert{mx}};

      \draw<3> (mn) -- (start) -- (mx);
      \onslide<3->{\movablecross{start}};
      \draw<3-> (start) node[above right] {$(k_2,\Pknotj{2})$};
      \draw<3> (mx) node[below left]     {$(k_{3},\Pknotj{{3}})$};
 
      \draw<4> (mn) -- (start) -- (mid) -- (mx);
      \onslide<4->{\movablecross{mid}};
      \draw<4-> (mid) node[below right] {$(k_3,\Pknotj{3})$};
      \draw<4> (mx) node[below left]     {$(k_{4},\Pknotj{{4}})$};

      \draw<5-> (mn) -- (start) -- (mid) -- (end) -- (mx);
      \onslide<5->{\movablecross{end}};
      \draw<5-> (end) node[above right] {$(k_4,\Pknotj{4})$};
      \draw<5-> (mx) node[below left]     {$(k_{\Nknots},\Pknotj{{\Nknots}})$};


      %\draw<2-> (\xmn,\ymn) coordinate (mn) -- (\xstart,\ystart) coordinate (start) -- (\xmid,\ymid) coordinate (mid) --  (\xend,\yend) coordinate(end) -- (\xmx,\ymx) coordinate(mx);

    % Draw the point labels
      %\draw<2-> (mn) node[below right]    {$(k_1,\Pknotj{1})$};
      %\draw<2-> (start) node[above right] {$(k_2,\Pknotj{2})$};
      %\draw<2-> (mid) node[below right]   {$(k_3,\Pknotj{3})$};
      %\draw<2-> (end) node[above right]   {$(k_4,\Pknotj{4})$};
      %\draw<2-> (mx) node[below left]     {$(k_{\Nknots},\Pknotj{{\Nknots}})$};

    % Draw a dashed line indicating the coordinate names
      %\draw[dashed] (yaxis |- start) node[left] {$y_{1}$}
      %-| (xaxis -| start) node[below] {$x_1$};
      %\draw[dashed] (yaxis |- mid) node[left] {$y_{2}$}
      %-| (xaxis -| mid) node[below] {$x_2$};
      %\draw[dashed] (yaxis |- end) node[left] {$y_{N}$}
      %-| (xaxis -| end) node[below] {$x_N$};
      %\draw  (xaxis -| start) node[below] {$\log_{10}k_2$};
      %\draw  (xaxis -| mid) node[below] {$\log_{10}k_3$};
      %\draw  (xaxis -| end) node[below] {$\log_{10}k_4$};

      % Draw the crosses
      %\onslide<2->{\movablevert{mn}
      %\movablecross{start}
      %\movablecross{mid}
      %\movablecross{end}
      %\movablevert{mx}
    %};

    % put some ellipses in between the start and end point

    \end{tikzpicture}

  }

\end{frame}


%\begin{frame}
%  \frametitle{Planck data}
%  \framesubtitle{Primordial power spectrum $\PR(k)$ reconstruction}
%  \begin{itemize}
%    \item<2-> Temperature data TT+lowP
%    \item<3-> Foreground $(14)$ \& cosmological $(4 +2*\Nknots-2)$  parameters
%    \item<4-> Marginalised plots of $\PR(k)$
%    \item<5->
%      \[ \prob(\PR|k,\Nknots) = \int \delta(\PR-f(k;\theta))\posterior(\theta)d\theta \]
%  \end{itemize}
%\end{frame}



\begin{frame}
  \frametitle<1>{0 internal knots}
  \frametitle<2>{1 internal knots}
  \frametitle<3>{2 internal knots}
  \frametitle<4>{3 internal knots}
  \frametitle<5>{4 internal knots}
  \frametitle<6>{5 internal knots}
  \frametitle<7>{6 internal knots}
  \frametitle<8>{7 internal knots}
  \frametitle<9>{8 internal knots}
  \frametitle<10>{Bayes Factors}
  \frametitle<11>{Marginalised plot}
  \framesubtitle{Primordial power spectrum $\PR(k)$ reconstruction}


  \begin{center}
    \includegraphics<1>[width=0.9\textwidth]{figures/0TT_fgivenx}
    \includegraphics<2>[width=0.9\textwidth]{figures/1TT_fgivenx}
    \includegraphics<3>[width=0.9\textwidth]{figures/2TT_fgivenx}
    \includegraphics<4>[width=0.9\textwidth]{figures/3TT_fgivenx}
    \includegraphics<5>[width=0.9\textwidth]{figures/4TT_fgivenx}
    \includegraphics<6>[width=0.9\textwidth]{figures/5TT_fgivenx}
    \includegraphics<7>[width=0.9\textwidth]{figures/6TT_fgivenx}
    \includegraphics<8>[width=0.9\textwidth]{figures/7TT_fgivenx}
    \includegraphics<9>[width=0.9\textwidth]{figures/8TT_fgivenx}
    \includegraphics<10>[width=0.9\textwidth]{figures/Bayes_TT.pdf}
    \includegraphics<11>[width=0.9\textwidth]{figures/combined_fgivenx.pdf}

  \end{center}
\end{frame}

\begin{frame}
  \frametitle{Object detection}
  \framesubtitle{Toy problem}

  \centerline{%
  \includegraphics[width=0.7\textwidth]{figures/object_detection_image}
}

\end{frame}

\begin{frame}
  \frametitle{Object detection}
  \framesubtitle{Evidences}

  \begin{itemize}
      \pause
    \item $\log\ev$ ratio: $-251:-156:-114:-117:-136$
      \pause
    \item odds ratio: $10^{-60}:10^{-19}:1:0.04:10^{-10}$
  \end{itemize}

\end{frame}

\begin{frame}
  \frametitle{PolyChord vs.\ MultiNest}
  \framesubtitle{Gaussian likelihood}

  \includegraphics<1>[width=\textwidth]{figures/polychord_vs_multinest.pdf}
  \includegraphics<2>[width=\textwidth]{figures/polychord_vs_multinest_1.pdf}
\end{frame}

\begin{frame}
  \frametitle{The future of nested sampling}
  \begin{itemize}
      \pause
    \item All previous approaches have focussed on trying to implement John's theoretical algorithm (still some work to be done here).
      \pause
    \item Nested sampling is really the first in a new class of ``probabilistic integration'' algorithms.
  \end{itemize}

\end{frame}

\begin{frame}
  \frametitle{Adjusting the meta-algorithm}
  \begin{itemize}
      \pause
    \item Need one sample from prior each time?
      \pause
    \item Adaptive numbers of live points?
      \pause
    \item Diffusive nested sampling?
      \pause
    \item Parallelisation?
  \end{itemize}

\end{frame}

\begin{frame}
  \frametitle{Conclusions}
  \framesubtitle{The future of nested sampling}

  \begin{itemize}
      \pause
    \item We are at the beginning of a new era of sampling algorithms
      \pause
    \item Plenty of more work in to be done in exploring new versions of nested sampling
      \pause
    \item Nested sampling is just the beginning
      \pause
    \item arXiv:1506.00171
      \pause
    \item \href{http://ccpforge.cse.rl.ac.uk/gf/project/polychord/}{http://ccpforge.cse.rl.ac.uk/gf/project/polychord/}
  \end{itemize}

\end{frame}


\end{document}
