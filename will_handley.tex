\documentclass[%
    %handout
]{beamer}
\usepackage{graphicx} % For including single page pdfs
\usepackage{bm}       % bold math
\usepackage{pgffor}   % for loop
\usepackage{tikz}
\usepackage{multimedia}
\usepackage{layouts}
\usepackage{hyperref}
\usepackage{cambridge_lecture}

% todo 
% - Ligo actual data
% -define IMRPhenom, EOBNR


\newcommand{\lik}{\mathcal{L}}
\newcommand{\posterior}{\mathcal{P}}
\newcommand{\prior}{\pi}
\newcommand{\ev}{\mathcal{Z}}

\newcommand{\prob}{\mathrm{P}}

\newcommand{\PR}{\mathcal{P}_\mathcal{R}}
\newcommand{\Pknotj}[1]{\mathcal{P}_{#1}}
\newcommand{\Nknots}{N_\text{knots}}
\newcommand{\nlive}{n_\text{live}}

\newcommand{\movablecross}[1]{%
  \draw[->](#1) -- ++(0:\croslen);
  \draw[->](#1) -- ++(90:\croslen);
  \draw[->](#1) -- ++(180:\croslen);
  \draw[->](#1) -- ++(270:\croslen);
  \fill[red!70!black] (#1) circle (2pt);
}

\newcommand{\movablevert}[1]{%
  \draw[->](#1) -- ++(90:\croslen);
  \draw[->](#1) -- ++(270:\croslen);
  \fill[red!70!black] (#1) circle (2pt);
}





\setbeamertemplate{navigation symbols}{} % Turn off that bottom bar


\title{Statistics}
\subtitle{Aachen Cosmotools 2018}
\author[Handley] % (optional, for multiple authors)
{Will Handley\\ \small{wh260@cam.ac.uk}}
\institute[University of Cambridge] % (optional)
{%
Astrophysics Group \\
Cavendish Laboratory \\
University of Cambridge
}
\date{April 24, 2018}

\include{include/beamer_commands}


\begin{document}

\begin{frame}
  \titlepage
\end{frame}

\include{include/further_reading}
\begin{frame}
    \frametitle{Introduction}
    \begin{itemize}
        \item Statistics $\equiv$ Inference $\equiv$ Machine Learning/AI.
        \item How to extract information about scientific models from data.
        \item Most cosmologists work in a {\em Bayesian\/} framework of inference, although {\em Frequentist\/} methods are also sometimes used.
    \end{itemize}
\end{frame}

\section{Fitting a line to data}
\begin{frame}
    \frametitle{Motivating example}
    \framesubtitle{Fitting lines to data}
    \begin{figright}[0.4]{./figures/data_points.pdf}
        \begin{itemize}
            \item We have noisy data $D$
            \item We wish to fit a model $M$
            \item Functional form $y=f_M(x;\theta)=a x + b$
            \item e.g:
                \begin{align}
                     f_\text{linear}(x;\theta)&=a x + b       \nonumber\\
                     f_\text{quadratic}(x;\theta)&=a x^2 + b  \nonumber
                \end{align}
            \item Model parameters $\theta= (a,b)$
        \end{itemize}
    \end{figright}
\end{frame}

\begin{frame}
    \frametitle{$\chi^2$ best-fit}
    \framesubtitle{Fitting lines to data}
    \begin{figright}[0.4]{./figures/data_diff.pdf}
        \begin{itemize}
            \item For each parameter set $\theta$:
                \[
                    \chi^2(\theta) = \sum_i \left|y_i - f(x_i;\theta)\right|^2
                \]
            \item Minimise $\chi^2$ wrt $\theta$
        \end{itemize}
    \end{figright}
\end{frame}

\begin{frame}
    \frametitle{$\chi^2$ with non-uniform data errors}
    \framesubtitle{Fitting lines to data}
    \begin{figright}[0.4]{./figures/data.pdf}
        \begin{itemize}
            \item If data have non-uniform errors:
                \[
                    \chi^2(\theta) = \sum_i \frac{\left|y_i - f(x_i;\theta)\right|^2}{\sigma_i^2}
                \]
        \end{itemize}
    \end{figright}
\end{frame}

\begin{frame}
    \frametitle{Problems with $\chi^2$}
    \framesubtitle{Fitting lines to data}
    \begin{figright}[0.4]{./figures/data_diff_2.pdf}
        \begin{itemize}
            \item How do we differentiate between models
            \item Why square the errors? -- could take absolute:
                \[
                    \psi^2(\theta) = \sum_i \frac{\left|y_i - f(x_i;\theta)\right|}{\sigma_i}
                \]
            \item Where does this even come from?
        \end{itemize}
    \end{figright}
\end{frame}

\begin{frame}
    \frametitle{Multivariate probability}
    \begin{itemize}
        \item Marginalisation:
            \begin{equation*}
                P(x) = \int P(x,y) dy
            \end{equation*}
        \item Conditioning:
            \begin{equation*}
                P(y|x) = \frac{P(x,y)}{P(x)} = \frac{P(x,y)}{\int P(x,y) dy}
            \end{equation*}
        \item De-Conditioning:
            \begin{equation*}
                P(x|y) P(y) = P(x,y)
            \end{equation*}
        \item Bayes theorem:
            \begin{equation*}
                P(y|x) = \frac{P(x|y) P(y)}{P(x)}
            \end{equation*}
            \begin{center}
                ``To flip a conditional $P(x|y)$, you first de-condition on $y$,\\ and then re-condition on $x$.''
            \end{center}
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Probability distributions}
    \framesubtitle{Fitting lines to data}
    \begin{figright}[0.6]{./figures/data_diff_1.pdf}
        \begin{itemize}
            \item The probability of observing a datum:
                \[
                    P(y_i | \theta,M) = \frac{1}{\sqrt{2\pi}\sigma_i}\exp\left({-\frac{|y_i-f(x_i;\theta)|^2}{2\sigma_i^2}}\right)
                \]
            \item The probability of observing the data:
                \begin{align}
                    P(D | \theta,M) &= \prod_i \frac{1}{\sqrt{2\pi}\sigma_i}\exp\left({-\frac{|y_i-f(x_i;\theta)|^2}{2\sigma_i^2}}\right) \nonumber\\
                    &=  \frac{1}{\prod_i\sqrt{2\pi}\sigma_i}\exp\sum_i{-\frac{|y_i-f(x_i;\theta)|^2}{2\sigma_i^2}} \nonumber\\
                    &\propto e^{-\chi^2(\theta)/2}
                    \nonumber
                \end{align}
        \end{itemize}
    \end{figright}
\end{frame}



\begin{frame}
    \frametitle{Maximum likelihood}
    \framesubtitle{Fitting lines to data}
    \begin{figleft}[0.6]{./figures/data_diff.pdf}
        \begin{itemize}
            \item Minimising $\chi^2(\theta)$  is equivalent to maximising $P(D|\theta,M) \propto e^{-\chi^2(\theta)/2}$
            \item $P(D|\theta,M)$ is called the Likelihood $L=L(\theta)$ of the parameters $\theta$
            \item ``Least squares'' $\equiv$ ``maximum likelihood'' \\(if data are gaussian).
        \end{itemize}
    \end{figleft}
\end{frame}

\begin{frame}
    \frametitle{Bayesian inference}
    \begin{itemize}
        \item Likelihood $L=P(D|\theta,M)$ is undeniably correct.
        \item Frequentists construct inference techniques purely from this function.
        \item The trend is cosmology is to work with a Bayesian approach.
        \item What we want are things like $P(\theta|D,M)$ and $P(M|D)$.
        \item To invert the conditionals, we need Bayes theorem:
            \begin{align}
                P(\theta|D,M) &= \frac{P(D|\theta,M) P(\theta|M)}{P(D|M)} \nonumber\\
                P(M|D) &= \frac{P(D|M) P(M)}{P(D)} \nonumber
            \end{align}
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Terminology}
    \framesubtitle{Bayesian inference}
    \begin{align}
        P(\theta|D,M) &= \frac{P(D|\theta,M) P(\theta|M)}{P(D|M)} \nonumber\\
        \text{Posterior} &= \frac{\text{Likelihood}\times\text{Prior}}{\text{Evidence}} \nonumber
    \end{align}
    \begin{align}
        P(M|D) &= \frac{P(D|M) P(M)}{P(D)} \nonumber\\
        \text{Model probability} &= \frac{\text{Evidence}\times\text{Model Prior}}{\text{Normalisation}} \nonumber
    \end{align}
\end{frame}

\begin{frame}
    \frametitle{The prior}
    \framesubtitle{Example: Biased coins}
    \begin{itemize}
        \item Need to define the \textbf{Prior} $P(\theta)$ --- probability of the bias, given no data
        \item Represents our knowledge of parameters before the data -- subjective
        \item Frequentists view this as a flaw in Bayesian inference. 
        \item Bayesians view this as an advantage
        \item Fundamental rule of Inference:\pause\\
            \vfill
            \begin{center}
                \Large You cannot extract information from data\\ without making assumptions 
            \end{center}
            \vfill
        \item All Bayesians do is make them explicit
        \item Any method that claims it is ``objective'' is simply hiding them
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Parameter estimation}
    \framesubtitle{Bayesian inference}
    \begin{figright}[0.3]{./figures/parameters.pdf}
        \begin{itemize}
            \item We may use $P(\theta|D,M)$ to inspect whether a model looks reasonable
        \end{itemize}
        \includegraphics[width=\textwidth]{./figures/data.pdf}
    \end{figright}
\end{frame}

\begin{frame}
    \frametitle{Predictive posterior}
    \begin{figright}[0.4]{./figures/fgivenx.pdf}
        More useful to plot:
        \begin{align}
        &P(y|x) = \nonumber\\
        &\int P(y|x,\theta) P(\theta) d\theta \nonumber
        \end{align}
        (all conditioned on $D,M$)
    \end{figright}
\end{frame}

\begin{frame}
    \frametitle{Model comparison}
    \framesubtitle{Bayesian inference}
    \begin{figright}[0.33]{./figures/evidences_log.pdf}
        \begin{itemize}
            \item We may use $Z=P(D|M)$ to determine whether a model is reasonable.
        \end{itemize}
    \end{figright}
\end{frame}
\begin{frame}
    \frametitle{Model comparison}
    \framesubtitle{Bayesian inference}
    \begin{figright}[0.33]{./figures/evidences_lin.pdf}
        \begin{itemize}
            \item We may use $Z=P(D|M)$ to determine whether a model is reasonable.
        \end{itemize}
    \end{figright}
\end{frame}

\begin{frame}
    \frametitle{Line fitting (context)}
    \begin{figright}[0.5]{./figures/supernovae.pdf}
        \begin{itemize}
            \item Whilst this model seems a little trite\ldots
            \item\ldots determining polynomial indices \\$\equiv$ determining cosmological material content:
        \end{itemize}
    \end{figright}
        \[
            {\left( \frac{H}{H_0} \right)}^2 = 
            \Omega_\text{r} {\left( \frac{a_0}{a} \right)}^4+
            \Omega_\text{m} {\left( \frac{a_0}{a} \right)}^3+
            \Omega_k {\left( \frac{a_0}{a} \right)}^2+
            \Omega_\Lambda
            \]
\end{frame}



\begin{frame}
    \frametitle{Probability distributions}

    \begin{itemize}
        \item As scientists, we are used to seeing error bars on results.
        \item Age of the universe ({\em Planck\/}): 
         \[13.73\pm 0.12\:\text{billion years old.}\]
        \item Masses of LIGO GW150914 binary merger: 
        \[m_1 = 39.4^{+5.5}_{-4.9}\:M_\odot,\qquad m_2 = 30.9^{+4.8}_{-4.4}\:M_\odot \]
        \item These are called {\em credible intervals}, state that we are e.g.\ $90\%$ confident of the value lying in this range.
        \item More importantly, these are {\em summary statistics}.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{LIGO binary merger}
    \begin{columns}
        \begin{column}{0.65\textwidth}
            \includegraphics[width=\textwidth]{./figures/ligo_m1_m2.pdf}
        \end{column}
        \begin{column}{0.35\textwidth}
            \begin{itemize}
                \item Summary statistics summarise a full probability distribution.
                \item One goal of inference is to produce these probability distributions.
            \end{itemize}
        \end{column}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Theory}
    \framesubtitle{Extended example of inference: LIGO}
    \includegraphics[width=\textwidth]{./figures/ligo_schematic.png}
\end{frame}

\begin{frame}
    \frametitle{The model $M$}
    \framesubtitle{Extended example of inference: LIGO}
    \includegraphics[width=\textwidth]{./figures/ligo_model.pdf}
\end{frame}

\begin{frame}
    \frametitle{The parameters $\Theta$ of the model $M$}
    \framesubtitle{Extended example of inference: LIGO}
    Theoretical signal depends on:
    \begin{itemize}
        \item $m_1, m_2$: mass of binary
        \item $\theta, \phi$: sky location
        \item $r$: luminosity distance 
        \item $\Phi_c, t_c$: phase and time of coalescence
        \item $i, \theta_\text{sky}$: inclination and angle on sky (orbital parameters)
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{The data $D$}
    \framesubtitle{Extended example of inference: LIGO}
    \includegraphics<1>[width=\textwidth]{./figures/ligo_data.pdf}
    \includegraphics<2>[width=\textwidth]{./figures/ligo_actual.png}
\end{frame}

\begin{frame}
    \frametitle<1>{The Likelihood: well matched}
    \frametitle<2>{The Likelihood: coalescence off}
    \frametitle<3>{The Likelihood: too large luminosity distance}
    \frametitle<4>{The Likelihood: incorrect inclination}
    \framesubtitle{Extended example of inference: LIGO}
    \only<1>{\includegraphics[width=\textwidth]{./figures/ligo_likelihood.pdf}}
    \only<2>{\includegraphics[width=\textwidth]{./figures/ligo_likelihood_t.pdf}}
    \only<3>{\includegraphics[width=\textwidth]{./figures/ligo_likelihood_r.pdf}}
    \only<4>{\includegraphics[width=\textwidth]{./figures/ligo_likelihood_i.pdf}}
\end{frame}



\begin{frame}
    \frametitle{Posterior $\mathcal{P}$}
    \framesubtitle{Extended example of inference: LIGO}
    \begin{itemize}
        \item Cannot plot the full posterior distribution:
            \[\mathcal{P}(\Theta) \equiv P(m_1,m_2,\theta,\phi,r,\Phi_c, t_c, i, \theta_\text{sky}|D,M)\]
        \item Can plot 1D and 2D {\em marginalised\/} distributions e.g:
            \begin{align}
            &P(m_1,m_2|D,M)=\nonumber\\&\int P(m_1,m_2,\theta,\phi,r,\Phi_c, t_c, i, \theta_\text{sky}|D,M) \,d\theta \,d\phi \,dr \,d\Phi_c \,d t_c \,d i \,d\theta_\text{sky}\nonumber
            \end{align}
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Posterior $\mathcal{P}$}
    \framesubtitle{Extended example of inference: LIGO}

    \begin{figleft}[0.65]{./figures/ligo_m1_m2.pdf}
        \begin{itemize}
            \item May do this for each pair of parameters
            \item Generates a {\em triangle plot}
        \end{itemize}
    \end{figleft}

\end{frame}

\begin{frame}
    \frametitle{Posterior $\mathcal{P}$}
    \framesubtitle{Extended example of inference: LIGO}
    \begin{figleft}[0.65]{./figures/ligo_full.pdf}
		\begin{itemize}
          \item Does give insight
          \item Not the full picture
		\end{itemize}
    \end{figleft}
\end{frame}

\begin{frame}
    \frametitle{Evidences and model comparison}
    \framesubtitle{Extended example of inference: LIGO}

    \begin{itemize}
        \item Up until now, we have discussed {\em Parameter estimation\/}: inferring what data tell us about parameters $\Theta$ of a model $M$.
        \item Scientifically speaking, this is only half the story.
        \item In general, we will have several competing models that describe the data, and we want to know which is the ``best''.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Parameter estimation}
    \framesubtitle{Another example.}

    \[\lik(\Theta) = P(D|\Theta,M)\]
    \begin{align}
        \onslide<2->{D =& \{C_\ell\only<6->{^\text{(Planck)}}\}} 
        \onslide<15->{+\{\text{LSS}\}} 
        \onslide<16->{+\{\text{``Big Data''}\}}
        \nonumber\\
        \onslide<3->{M =& \Lambda\text{CDM}} 
        \onslide<9->{+ \text{extensions} }
        \nonumber\\
        \onslide<4->{\Theta =& \Theta_{\Lambda \text{CDM}}} \onslide<7->{+ \Theta_\text{Planck}} \onslide<10->{+ \Theta_\text{extensions}}\nonumber\\
        \onslide<5->{\Theta_{\Lambda \text{CDM}} =& ( \Omega_b h^2, \Omega_c h^2, 100\theta_{MC}, \tau, {\rm{ln}}(10^{10} A_s), n_s) \nonumber\\}
        \onslide<8->{\Theta_\text{Planck} =& (y_{\rm cal}, A^{CIB}_{217}, \xi^{tSZ-CIB}, A^{tSZ}_{143}, A^{PS}_{100}, A^{PS}_{143}, A^{PS}_{143\times 217}, A^{PS}_{217},\nonumber\\
        &A^{kSZ}, A^{{\rm dust}TT}_{100}, A^{{\rm dust}TT}_{143}, A^{{\rm dust}TT}_{143\times 217}, A^{{\rm dust}TT}_{217}, c_{100}, c_{217}) \nonumber\\}
        \onslide<11->{\Theta_\text{extensions} =& (
                n_{\rm run}
                \only<12->{,n_{\rm run,run}}
                \only<13->{,w}
                \only<14->{,\Sigma m_\nu, m_{\nu,{\rm{sterile}}}^{\rm{eff}}}
        ) \nonumber}
    \end{align}

    \begin{itemize}
        \item<17->{Parameter estimation: $L, \pi \to \mathcal{P}$: model parameters}
        \item<17->{Model comparison: $L, \pi \to Z$: how good model is}
    \end{itemize}

\end{frame}


\begin{frame}
    \frametitle{Sampling}
    \framesubtitle{How to describe a high-dimensional posterior}

	\begin{figright}{./figures/ligo_m1_m2.pdf}
		\begin{itemize}
          \item In high dimensions, posterior $\posterior$ occupies a vanishingly small region of the prior $\prior$.
          \item {\em Sampling\/} the posterior is an excellent compression scheme.
		\end{itemize}
	\end{figright}
 
\end{frame}
%
\begin{frame}
    \frametitle{Why do sampling?}
    \framesubtitle{Marginalisation over the posterior}

    \begin{itemize}
        \item Set of $N$ samples $S = \{\Theta^{(i)}: i=1,\ldots N:\: \Theta^{(i)}\sim\mathcal{P}\}$
        \item Mean mass: \[
                \bar{m}_1 \equiv\langle m_1\rangle_\mathcal{P} \approx 
                \only<1>{\frac{1}{N}\sum_{i=1}^N m_1^{(i)}}
                \only<2>{\frac{\sum_{i=1}^N w^{(i)} m_1^{(i)}}{\sum_{i=1}^N w^{(i)}}}
            \]
        \item Mass covariance: \[
                \mathrm{Cov}(m_1,m_2) \approx 
                \only<1>{\frac{1}{N}\sum_{i=1}^N (m_1^{(i)}-\bar{m}_1)(m_2^{(i)}-\bar{m}_2)}
                \only<2>{\frac{\sum_{i=1}^N (m_1^{(i)}-\bar{m}_1)(m_2^{(i)}-\bar{m}_2)}{\sum_{i=1}^N w^{(i)}}}
            \]
        \item Marginalised samples: Just ignore the other coordinates.
        \item N.B. Typically have {\em weighted\/} samples
    \end{itemize}
\end{frame}
%
\begin{frame}
    \frametitle{Parameter estimation}
    \begin{itemize}
        \item The name of the game is therefore drawing samples $S$ from the posterior $\mathcal{P}$ with the minimum number of likelihood calls.
        \item Gridding is doomed to failure in high dimensions.
        \item Enter Metropolis Hastings.
    \end{itemize}
\end{frame}



\section{Metropolis Hastings}

\begin{frame}
    \frametitle{Current sampling approaches}
    \begin{enumerate}
        \item Metropolis Hastings.
        \item Hamiltonian Monte-Carlo (HMC).
        \item Ensemble sampling (e.g.\ emcee).
    \end{enumerate}
\end{frame}

\begin{frame}
  \frametitle{Metropolis Hastings} 
  \begin{itemize}
      
    \item Turn the $N$-dimensional problem into a one-dimensional one.
      \begin{enumerate}
          
        \item Pick random direction
          
        \item Choose step length
          
        \item If uphill, make step\ldots
          
        \item \ldots otherwise sometimes make step. 
      \end{enumerate}
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Metropolis Hastings} 
  \includegraphics[width=\textwidth]{movies/MCMC_0.pdf}
\end{frame}
\begin{frame}
  \frametitle{Metropolis Hastings} 
  \movie[width=\textwidth,height=0.52\textwidth]{}{movies/MCMC.mp4}
\end{frame}
\begin{frame}
  \frametitle{Metropolis Hastings} 
  \includegraphics[width=\textwidth]{movies/MCMC_1.pdf}
\end{frame}




\begin{frame}
  \frametitle{Metropolis Hastings} 
  \framesubtitle{Struggles with\ldots}
  \pause
  \begin{enumerate}
      \item Burn in
      \item Multimodality
      \item Correlated Peaks
      \item Phase transitions
  \end{enumerate}
\end{frame}

\begin{frame}
  \frametitle{Hamiltonian Monte-Carlo} 
  \begin{itemize}
      \item Key idea: Treat $\log L(\Theta)$ as a potential energy
      \item Guide walker under ``force'': \[F(\Theta) =\nabla \log L(\Theta)\]
      \item Walker is naturally ``guided'' uphill
      \item Conserved quantities mean efficient acceptance ratios.
      \item stan is a fully fledged, rapidly developing programming language with HMC as a default sampler.
  \end{itemize}
\end{frame}


\begin{frame}
  \frametitle{Ensemble sampling} 
  \begin{itemize}
      \item Instead of one walker, evolve a set of $n$ walkers.
      \item Can use information present in ensemble to guide proposals.
      \item emcee: affine invariant proposals.
      \item emcee is not the only (or even best) affine invariant approach.
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{The fundamental issue with all of the above} 

  \begin{itemize}
    \item They don't give you evidences!
  \end{itemize}

  \begin{align}
    \ev 
    &= \prob(D|M) 
    \nonumber\\
    &= \int\prob(D|\Theta,M)\prob(\Theta|M) d\Theta 
    \nonumber\\
    &= \left\langle \lik \right\rangle_\prior
    \nonumber
  \end{align}
  
  \begin{itemize}
    \item MCMC fundamentally explores the posterior, and cannot average over the prior.
    \item Simulated annealing gives one possibility for computing evidences.
    \begin{itemize}
        \item Inspired by thermodynamics.
        \item Suffers from similar issues to MCMC.
        \item Unclear how to choose correct annealing schedule
    \end{itemize}
  \end{itemize}
 
\end{frame}

\section{Nested Sampling}

\begin{frame}
  \frametitle{What is nested sampling?}
  \begin{itemize}
    \item Nested sampling is an alternative way of sampling posteriors. 
    \item Uses ensemble sampling to compress prior to posterior.
    \item In doing so, it circumvents many issues (dimensionality, topology, geometry) that beset standard approaches.
  \end{itemize}
\end{frame}


\begin{frame}
  \frametitle{Nested Sampling} 
  \framesubtitle{John Skilling's alternative to traditional MCMC!} 

  
  New procedure: 

  
  Maintain a set $S$ of $n$ samples, which are sequentially updated:

  \begin{description}
      
    \item[$S_0$:] Generate $n$ samples uniformly over the space (from the prior $\prior$). 
      
    \item[$S_{n+1}$:] Delete the lowest likelihood sample in $S_{n}$, and replace it with a new uniform sample with higher likelihood
  \end{description}

  
  Requires one to be able to uniformly within a region, subject to a {\em hard likelihood constraint}.

\end{frame}



\begin{frame}
  \frametitle{Nested Sampling}
  \framesubtitle{Graphical aid}
\foreach \pagenum in {1,...,38} {%
  \includegraphics<\pagenum>[width=\textwidth,page=\pagenum]{figures/nested_sampling}
}
\end{frame}


%%\begin{frame}
%%  \frametitle{Nested sampling}
%%  \includegraphics[width=\textwidth]{movies/NS_0.pdf}
%%\end{frame}
%%\begin{frame}
%%  \frametitle{Nested sampling}
%%  \movie[width=\textwidth,height=0.52\textwidth]{}{movies/NS.mp4}
%%\end{frame}
%%\begin{frame}
%%  \frametitle{Nested sampling}
%%  \includegraphics[width=\textwidth]{movies/NS_1.pdf}
%%\end{frame}
%
%
%
%
%%\begin{frame}
%%  \frametitle{Estimating evidences} 
%%  \framesubtitle{Evidence error} 
%%
%%\foreach \pagenum in {1,...,9} {%
%%  \includegraphics<\pagenum>[width=\textwidth,page=\pagenum]{figures/areas}
%%}
%% 
%%\end{frame}
%%
%%\begin{frame}
%%  \frametitle{Estimating evidences} 
%%  \framesubtitle{Evidence error} 
%%
%%
%%  \begin{itemize}
%%    \item<2-> approximate compression:
%%  \end{itemize}
%%  \[ 
%%    \onslide<2->{\Delta \log X \sim -\frac{1}{n}} 
%%    \onslide<3->{\pm \frac{1}{n}} 
%%  \]
%%  \[ 
%%    \onslide<4->{\log X_i \sim -\frac{i}{n}}
%%    \onslide<5->{\pm \frac{\sqrt{i}}{n}} 
%%  \]
%%  \onslide<6->{%
%%  \begin{itemize}
%%    \item<6-> \# of steps to get to $H$:
%%  \end{itemize}
%%  \[ i_H \sim n H \]
%%  }
%%  \onslide<7->{%
%%  \begin{itemize}
%%    \item estimate of volume at $H$:
%%  \end{itemize}
%%  \[ \log X_H \approx -H \pm \sqrt{\frac{H}{n}} \]
%%}
%%
%%  \onslide<8->{%
%%  \begin{itemize}
%%    \item estimate of evidence error:
%%  \end{itemize}
%%  \[ \log\ev \approx \sum w_i \lik_i  \pm \sqrt{\frac{H}{n}} \]
%%}
%%
%%\end{frame}
%
%
\begin{frame}
  \frametitle{Nested sampling} 

  \begin{itemize}
    \item The set of dead points are posterior samples with an appropriate weighting factor
    \item They can also be used to calculate evidences, since it sequentially updates the priors.
  \end{itemize}
 
\end{frame}


\begin{frame}
  \frametitle{Sampling from a hard likelihood constraint} 

  
  \begin{quote}
    ``It is not the purpose of this introductory paper to develop the technology of navigation within such a volume. We merely note that exploring a hard-edged likelihood-constrained domain should prove to be neither more nor less demanding than exploring a likelihood-weighted space.''
    
   {\hfill --- John Skilling}
  \end{quote}

  \begin{itemize}
      
    \item Most of the work in NS to date has been in attempting to implement a hard-edged sampler in the NS meta-algorithm.
  \end{itemize}
 
\end{frame}


\begin{frame}
  \frametitle{Sampling within an iso-likelihood contour}
  \framesubtitle{Previous attempts}


  \begin{description}
    \item[Rejection Sampling] MultiNest; F.\ Feroz \& M.\ Hobson (2009).
      \begin{itemize}
        \item Suffers in high dimensions
      \end{itemize}
      \item[Hamiltonian] M.J. Betancourt (2010) 
      \item[Galilean] F.\ Feroz \& J.\ Skilling (2013) 
      \begin{itemize}
        \item Requires gradients and tuning
      \end{itemize}
      \item[Diffusive Nested Sampling] B.\ Brewer et al.\ (2009,2016).
      \begin{itemize}
        \item Very promising
        \item Still needs tuning.
      \end{itemize}
      \item[Slice Sampling] PolyChord; Handley et al.\ (2015).
      \begin{itemize}
          \item Current ``state-of-the-art''.
          \item PolyChord 2.0 imminent.
      \end{itemize}
  \end{description}

\end{frame}

%
%\section{Examples}
%\begin{frame}
%  \frametitle{Object detection}
%  \framesubtitle{Toy problem}
%
%  \centerline{%
%  \includegraphics<1>[width=\textwidth]{figures/object_detection_image}
%  \includegraphics<2>[width=\textwidth]{figures/object_detection_image_no_noise}
%}
%
%\end{frame}
%
\begin{frame}
  \frametitle{PolyChord in action}
  \framesubtitle{Primordial power spectrum $\PR(k)$ reconstruction}


  \resizebox{\textwidth} {!} {%
    \begin{tikzpicture}
    % width of axes
      \def\xwidth{7}
      \def\ywidth{4}
    % min coordinate
      \def\xmn{0.5}
      \def\ymn{2}
    % start coordinate
      \def\xstart{2}
      \def\ystart{3}
    % middle coordinate
      \def\xmid{3}
      \def\ymid{1}
    % end coordinate
      \def\xend{5.5}
      \def\yend{3}
    % max coordinate
      \def\xmx{6.5}
      \def\ymx{1.5}

    % length of crosses
      \def\croslen{0.4}


    % Draw axes
      \draw [<->,thick] (0,\ywidth) node (yaxis) [above] {$\log\PR(k)$}
      |- (\xwidth,0) node (xaxis) [right] {$\log k$};
    % Draw limits
      %\draw [-,dashed] (\xmn,0) node[below] {$\log_{10}k_1$} -- (\xmn,\ywidth) ;
      %\draw [-,dashed] (\xmx,0) node[below] {$\log_{10}k_N$} -- (\xmx,\ywidth) ;

      \draw<1> (\xmn,\ymn) -- (\xmx,\ymx);
      \draw<1> (\xstart,\ystart) node[below right] {$A_s {\left(\frac{k}{k_*}\right)}^{n_s-1}$};

    % Draw the line joining start and end

      \coordinate (mn) at (\xmn,\ymn);
      \coordinate (start) at (\xstart,\ystart);
      \coordinate (mid) at (\xmid,\ymid);
      \coordinate (end) at (\xend,\yend);
      \coordinate (mx) at (\xmx,\ymx);
      \draw<2> (mn) -- (mx);
      \draw<2-> (mn) node[below right]    {$(k_1,\Pknotj{1})$};
      \draw<2> (mx) node[below left]     {$(k_{2},\Pknotj{{2}})$};
      \onslide<2->{\movablevert{mn}};
      \onslide<2->{\movablevert{mx}};

      \draw<3> (mn) -- (start) -- (mx);
      \onslide<3->{\movablecross{start}};
      \draw<3-> (start) node[above right] {$(k_2,\Pknotj{2})$};
      \draw<3> (mx) node[below left]     {$(k_{3},\Pknotj{{3}})$};
 
      \draw<4> (mn) -- (start) -- (mid) -- (mx);
      \onslide<4->{\movablecross{mid}};
      \draw<4-> (mid) node[below right] {$(k_3,\Pknotj{3})$};
      \draw<4> (mx) node[below left]     {$(k_{4},\Pknotj{{4}})$};

      \draw<5-> (mn) -- (start) -- (mid) -- (end) -- (mx);
      \onslide<5->{\movablecross{end}};
      \draw<5-> (end) node[above right] {$(k_4,\Pknotj{4})$};
      \draw<5-> (mx) node[below left]     {$(k_{\Nknots},\Pknotj{{\Nknots}})$};


      %\draw<2-> (\xmn,\ymn) coordinate (mn) -- (\xstart,\ystart) coordinate (start) -- (\xmid,\ymid) coordinate (mid) --  (\xend,\yend) coordinate(end) -- (\xmx,\ymx) coordinate(mx);

    % Draw the point labels
      %\draw<2-> (mn) node[below right]    {$(k_1,\Pknotj{1})$};
      %\draw<2-> (start) node[above right] {$(k_2,\Pknotj{2})$};
      %\draw<2-> (mid) node[below right]   {$(k_3,\Pknotj{3})$};
      %\draw<2-> (end) node[above right]   {$(k_4,\Pknotj{4})$};
      %\draw<2-> (mx) node[below left]     {$(k_{\Nknots},\Pknotj{{\Nknots}})$};

    % Draw a dashed line indicating the coordinate names
      %\draw[dashed] (yaxis |- start) node[left] {$y_{1}$}
      %-| (xaxis -| start) node[below] {$x_1$};
      %\draw[dashed] (yaxis |- mid) node[left] {$y_{2}$}
      %-| (xaxis -| mid) node[below] {$x_2$};
      %\draw[dashed] (yaxis |- end) node[left] {$y_{N}$}
      %-| (xaxis -| end) node[below] {$x_N$};
      %\draw  (xaxis -| start) node[below] {$\log_{10}k_2$};
      %\draw  (xaxis -| mid) node[below] {$\log_{10}k_3$};
      %\draw  (xaxis -| end) node[below] {$\log_{10}k_4$};

      % Draw the crosses
      %\onslide<2->{\movablevert{mn}
      %\movablecross{start}
      %\movablecross{mid}
      %\movablecross{end}
      %\movablevert{mx}
    %};

    % put some ellipses in between the start and end point

    \end{tikzpicture}

  }

\end{frame}
%
%
%%\begin{frame}
%%  \frametitle{Planck data}
%%  \framesubtitle{Primordial power spectrum $\PR(k)$ reconstruction}
%%  \begin{itemize}
%%    \item<2-> Temperature data TT+lowP
%%    \item<3-> Foreground $(14)$ \& cosmological $(4 +2*\Nknots-2)$  parameters
%%    \item<4-> Marginalised plots of $\PR(k)$
%%    \item<5->
%%      \[ \prob(\PR|k,\Nknots) = \int \delta(\PR-f(k;\theta))\posterior(\theta)d\theta \]
%%  \end{itemize}
%%\end{frame}
%
%
%
\begin{frame}
  \frametitle<1>{0 internal knots}
  \frametitle<2>{1 internal knots}
  \frametitle<3>{2 internal knots}
  \frametitle<4>{3 internal knots}
  \frametitle<5>{4 internal knots}
  \frametitle<6>{5 internal knots}
  \frametitle<7>{6 internal knots}
  \frametitle<8>{7 internal knots}
  \frametitle<9>{8 internal knots}
  \frametitle<10>{Bayes Factors}
  \frametitle<11>{Marginalised plot}
  \framesubtitle{Primordial power spectrum $\PR(k)$ reconstruction}


  \begin{center}
    \includegraphics<1>[width=0.8\textwidth]{figures/0TT_fgivenx}
    \includegraphics<2>[width=0.8\textwidth]{figures/1TT_fgivenx}
    \includegraphics<3>[width=0.8\textwidth]{figures/2TT_fgivenx}
    \includegraphics<4>[width=0.8\textwidth]{figures/3TT_fgivenx}
    \includegraphics<5>[width=0.8\textwidth]{figures/4TT_fgivenx}
    \includegraphics<6>[width=0.8\textwidth]{figures/5TT_fgivenx}
    \includegraphics<7>[width=0.8\textwidth]{figures/6TT_fgivenx}
    \includegraphics<8>[width=0.8\textwidth]{figures/7TT_fgivenx}
    \includegraphics<9>[width=0.8\textwidth]{figures/8TT_fgivenx}
    \includegraphics<10>[width=0.8\textwidth]{figures/Bayes_TT.pdf}
    \includegraphics<11>[width=0.8\textwidth]{figures/combined_fgivenx.pdf}

  \end{center}
\end{frame}
\begin{frame}
  \frametitle<1>{COBE (pre-2002)}
  \frametitle<2>{COBE et al (2002)}
  \frametitle<3>{WMAP (2012)}
  \frametitle<4>{Planck (2013)}
  \frametitle<5>{Planck (2015)}
  \framesubtitle{Primordial power spectrum $\PR(k)$ reconstruction}


  \begin{center}
    \includegraphics<1>[width=0.6\textwidth]{figures/cobe}
    \includegraphics<2>[width=0.6\textwidth]{figures/pre_WMAP}
    \includegraphics<3>[width=0.6\textwidth]{figures/WMAP}
    \includegraphics<4>[width=0.6\textwidth]{figures/planck_2013}
    \includegraphics<5>[width=0.6\textwidth]{figures/planck_2015}

  \end{center}
\end{frame}

%
%\begin{frame}
%    \frametitle{Dark energy equation of state reconstruction}
%    \begin{itemize}
%        \item Same thing, but for Dark energy equation of state $w(z)$ (quintessence).
%        \item Data used is Planck 2015, BOSS DR 11, JLA supernovae and BOSS Ly$\alpha$ data
%    \end{itemize}
%\end{frame}
%\begin{frame}
%    \frametitle<1>{Flat, variable $w$}
%    \frametitle<2>{Tilted}
%    \frametitle<3>{1 internal node}
%    \frametitle<4>{2 internal nodes}
%    \frametitle<5>{3 internal nodes}
%    \frametitle<6>{Marginalised plot - just extension models}
%    \frametitle<7>{Marginalised plot - including LCDM}
%    \framesubtitle{Dark energy equation of state reconstruction}
%
%    \begin{center}
%        \includegraphics<1>[width=0.9\textwidth]{figures/wCDM_1000Nlive_wgiivenz.pdf}
%        \includegraphics<2>[width=0.9\textwidth]{figures/tCDM_100Nlive_wgiivenz.pdf}
%        \includegraphics<3>[width=0.9\textwidth]{figures/1CDM_1000Nlive_wgiivenz.pdf}
%        \includegraphics<4>[width=0.9\textwidth]{figures/2CDM_1000Nlive_wgiivenz.pdf}
%        \includegraphics<5>[width=0.9\textwidth]{figures/3CDM_1000Nlive_wgiivenz.pdf}
%        \includegraphics<6>[width=0.9\textwidth]{figures/extensionModels_1000Nlive_wgivenz.pdf}
%        \includegraphics<7>[width=0.9\textwidth]{figures/allModels_1000Nlive_wgivenz.pdf}
%    \end{center}
%\end{frame}
%
%
%\begin{frame}
%  \frametitle{Nested Sampling}
%  \framesubtitle{Calculating evidences}
%  \foreach \pagenum in {1,...,16} {%
%      \includegraphics<\pagenum>[width=\textwidth,page=\pagenum]{figures/lesbesgue}
%  }
%\end{frame}
%
%
%
%\begin{frame}
%  \frametitle{Nested Sampling} 
%  \framesubtitle{Exponential volume contraction} 
%  
%  \begin{itemize}
%      
%    \item At each iteration, the likelihood contour will shrink in volume by  $\approx 1/n$.
%      
%    \item Nested sampling zooms in to the peak of the posterior {\em exponentially}.
%      
%    \item 
%      \begin{equation}
%          \ev \approx \sum_i \Delta\lik_i X_{i}
%      \end{equation}
%      
%    \item 
%      \begin{equation}
%        X_{i+1} \approx \frac{n}{n+1}X_i, \qquad X_{0} = 1
%      \end{equation}
%  \end{itemize}
%  
%\end{frame}

\include{include/further_reading}

\end{document}
